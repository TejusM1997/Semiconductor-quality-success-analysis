{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-07-19 11:55:00</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-07-19 12:32:00</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>...</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-07-19 13:17:00</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>...</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-07-19 14:43:00</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>...</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-07-19 15:22:00</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 592 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Time        0        1          2          3       4      5  \\\n",
       "0  2008-07-19 11:55:00  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   \n",
       "1  2008-07-19 12:32:00  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0   \n",
       "2  2008-07-19 13:17:00  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   \n",
       "3  2008-07-19 14:43:00  2988.72  2479.90  2199.0333   909.7926  1.3204  100.0   \n",
       "4  2008-07-19 15:22:00  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0   \n",
       "\n",
       "          6       7       8  ...       581     582     583     584      585  \\\n",
       "0   97.6133  0.1242  1.5005  ...       NaN  0.5005  0.0118  0.0035   2.3630   \n",
       "1  102.3433  0.1247  1.4966  ...  208.2045  0.5019  0.0223  0.0055   4.4447   \n",
       "2   95.4878  0.1241  1.4436  ...   82.8602  0.4958  0.0157  0.0039   3.1745   \n",
       "3  104.2367  0.1217  1.4882  ...   73.8432  0.4990  0.0103  0.0025   2.0544   \n",
       "4  100.3967  0.1235  1.5031  ...       NaN  0.4800  0.4766  0.1045  99.3032   \n",
       "\n",
       "      586     587     588       589  Pass/Fail  \n",
       "0     NaN     NaN     NaN       NaN         -1  \n",
       "1  0.0096  0.0201  0.0060  208.2045         -1  \n",
       "2  0.0584  0.0484  0.0148   82.8602          1  \n",
       "3  0.0202  0.0149  0.0044   73.8432         -1  \n",
       "4  0.0202  0.0149  0.0044   73.8432         -1  \n",
       "\n",
       "[5 rows x 592 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('uci-secom.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_values(df):\n",
    "    \"\"\"a function to show null values with percentage\"\"\"\n",
    "    nv=pd.concat([df.isnull().sum(), 100 * df.isnull().sum()/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n",
    "    return nv[nv.Missing_Records>0].sort_values('Missing_Records', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing_Records</th>\n",
       "      <th>Percentage (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>1429</td>\n",
       "      <td>91.193363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>1429</td>\n",
       "      <td>91.193363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1429</td>\n",
       "      <td>91.193363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1429</td>\n",
       "      <td>91.193363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>1341</td>\n",
       "      <td>85.577537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>1</td>\n",
       "      <td>0.063816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1</td>\n",
       "      <td>0.063816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1</td>\n",
       "      <td>0.063816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>1</td>\n",
       "      <td>0.063816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1</td>\n",
       "      <td>0.063816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>538 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Missing_Records  Percentage (%)\n",
       "292             1429       91.193363\n",
       "293             1429       91.193363\n",
       "157             1429       91.193363\n",
       "158             1429       91.193363\n",
       "358             1341       85.577537\n",
       "..               ...             ...\n",
       "456                1        0.063816\n",
       "218                1        0.063816\n",
       "356                1        0.063816\n",
       "457                1        0.063816\n",
       "589                1        0.063816\n",
       "\n",
       "[538 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_na=null_values(df)\n",
    "df_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing_Records</th>\n",
       "      <th>Percentage (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>1429</td>\n",
       "      <td>91.193363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>1429</td>\n",
       "      <td>91.193363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1429</td>\n",
       "      <td>91.193363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1429</td>\n",
       "      <td>91.193363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>1341</td>\n",
       "      <td>85.577537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1341</td>\n",
       "      <td>85.577537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>1341</td>\n",
       "      <td>85.577537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>1341</td>\n",
       "      <td>85.577537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1018</td>\n",
       "      <td>64.964901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>949</td>\n",
       "      <td>60.561583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>949</td>\n",
       "      <td>60.561583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>949</td>\n",
       "      <td>60.561583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>949</td>\n",
       "      <td>60.561583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>794</td>\n",
       "      <td>50.670070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>794</td>\n",
       "      <td>50.670070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>794</td>\n",
       "      <td>50.670070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>794</td>\n",
       "      <td>50.670070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Missing_Records  Percentage (%)\n",
       "292             1429       91.193363\n",
       "293             1429       91.193363\n",
       "157             1429       91.193363\n",
       "158             1429       91.193363\n",
       "358             1341       85.577537\n",
       "85              1341       85.577537\n",
       "492             1341       85.577537\n",
       "220             1341       85.577537\n",
       "518             1018       64.964901\n",
       "246             1018       64.964901\n",
       "245             1018       64.964901\n",
       "516             1018       64.964901\n",
       "517             1018       64.964901\n",
       "110             1018       64.964901\n",
       "384             1018       64.964901\n",
       "382             1018       64.964901\n",
       "383             1018       64.964901\n",
       "109             1018       64.964901\n",
       "244             1018       64.964901\n",
       "111             1018       64.964901\n",
       "580              949       60.561583\n",
       "578              949       60.561583\n",
       "581              949       60.561583\n",
       "579              949       60.561583\n",
       "73               794       50.670070\n",
       "72               794       50.670070\n",
       "345              794       50.670070\n",
       "346              794       50.670070"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_na=df_na[df_na['Percentage (%)']>50]\n",
    "df_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be said that 91.18% of the features have missing records,so here we shall ascertain the features which have more than 50% missing values because even imputation wont help if more than 50% of data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These 28 features must be dropped\n",
    "df_na.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 564)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop(axis=1,columns=df_na.index)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the features with missing records less than 50% we shall use KNN imputer for filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see the sklearn documentation for KNN imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer= KNNImputer(n_neighbors=3,weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperte independent and dependent features\n",
    "X=df.drop('Pass/Fail',axis=1)\n",
    "y=df['Pass/Fail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop('Time',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(X)\n",
    "X_clean=imputer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6765</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>0.016581</td>\n",
       "      <td>0.01377</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>78.368624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1065</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.02010</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>208.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0952</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.04840</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>82.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7585</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.01490</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>73.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6597</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.01490</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>73.843200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1          2          3       4      5         6       7    \\\n",
       "0  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   97.6133  0.1242   \n",
       "1  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
       "2  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   95.4878  0.1241   \n",
       "3  2988.72  2479.90  2199.0333   909.7926  1.3204  100.0  104.2367  0.1217   \n",
       "4  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0  100.3967  0.1235   \n",
       "\n",
       "      8       9    ...     552      553     554     555     556      557  \\\n",
       "0  1.5005  0.0162  ...  1.6765  14.9509  0.5005  0.0118  0.0035   2.3630   \n",
       "1  1.4966 -0.0005  ...  1.1065  10.9003  0.5019  0.0223  0.0055   4.4447   \n",
       "2  1.4436  0.0041  ...  2.0952   9.2721  0.4958  0.0157  0.0039   3.1745   \n",
       "3  1.4882 -0.0124  ...  1.7585   8.5831  0.4990  0.0103  0.0025   2.0544   \n",
       "4  1.5031 -0.0031  ...  1.6597  10.9698  0.4800  0.4766  0.1045  99.3032   \n",
       "\n",
       "        558      559       560         561  \n",
       "0  0.016581  0.01377  0.004687   78.368624  \n",
       "1  0.009600  0.02010  0.006000  208.204500  \n",
       "2  0.058400  0.04840  0.014800   82.860200  \n",
       "3  0.020200  0.01490  0.004400   73.843200  \n",
       "4  0.020200  0.01490  0.004400   73.843200  \n",
       "\n",
       "[5 rows x 562 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=pd.DataFrame(X_clean)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Pass or Fail')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df['Pass/Fail'])\n",
    "plt.xlabel('Class labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Pass or Fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Class imbalance is too high in favour of Passing(encoded as-1), therefore any model would give us a biased response towards -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,stratify=y,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    302\n",
       " 1    159\n",
       "Name: Pass/Fail, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled=pd.DataFrame(scaler.fit_transform(X_train),index=X_train.index)\n",
    "X_test_scaled=pd.DataFrame(scaler.fit_transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the strategy implies that the minority class wil half the number of labels the majority class has\n",
    "oversample= SMOTE(sampling_strategy=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset distribution: Counter({-1: 1023, 1: 73})\n",
      "Resampled dataset distribution: Counter({-1: 1023, 1: 511})\n"
     ]
    }
   ],
   "source": [
    "print ('Original dataset distribution: {}'.format(Counter(y_train)))\n",
    "X_res,y_res=oversample.fit_resample(X_train_scaled,y_train)\n",
    "print ('Resampled dataset distribution: {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler \n",
    "rand_oversamp= RandomOverSampler(sampling_strategy='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset distribution: Counter({-1: 1023, 1: 73})\n",
      "Resampled dataset distribution: Counter({-1: 1023, 1: 1023})\n"
     ]
    }
   ],
   "source": [
    "print ('Original dataset distribution: {}'.format(Counter(y_train)))\n",
    "X_res1,y_res1=rand_oversamp.fit_resample(X_train_scaled,y_train)\n",
    "print ('Resampled dataset distribution: {}'.format(Counter(y_res1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I observed is that both these techniques are yielding the same indistinguishable outcomes, nevertheless if we choose a sample proportion similar to 10:10 then we might get a model that might not generalize well on the test set , hence the ratio of 10:5 is preferrable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First round we will create preliminary classifier models then based on those results we can determine if hyper parameter optimization is warranted or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr= LogisticRegression()\n",
    "lr.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models accuracy score is 0.8428874734607219\n"
     ]
    }
   ],
   "source": [
    "print('The models accuracy score is', accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[392  26]\n",
      " [ 48   5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.94      0.91       418\n",
      "           1       0.16      0.09      0.12        53\n",
      "\n",
      "    accuracy                           0.84       471\n",
      "   macro avg       0.53      0.52      0.52       471\n",
      "weighted avg       0.81      0.84      0.82       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred,y_test))\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) XG Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb= XGBClassifier()\n",
    "xgb.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1= xgb.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models accuracy is 0.9150743099787686\n"
     ]
    }
   ],
   "source": [
    "print('The models accuracy is',accuracy_score(y_pred1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[429  29]\n",
      " [ 11   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      0.94      0.96       458\n",
      "           1       0.06      0.15      0.09        13\n",
      "\n",
      "    accuracy                           0.92       471\n",
      "   macro avg       0.52      0.55      0.52       471\n",
      "weighted avg       0.95      0.92      0.93       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred1,y_test))\n",
    "print(classification_report(y_pred1,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc= SVC()\n",
    "svc.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2= svc.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models accuracy is 0.9256900212314225\n"
     ]
    }
   ],
   "source": [
    "print('The models accuracy is', accuracy_score(y_pred2,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[436  31]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.93      0.96       467\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.93       471\n",
      "   macro avg       0.50      0.47      0.48       471\n",
      "weighted avg       0.98      0.93      0.95       471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred2,y_test))\n",
    "print(classification_report(y_pred2,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite suspicious that no data point is classified under rejected class, therfore we shall see if the model is biased towards the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986962190352021\n"
     ]
    }
   ],
   "source": [
    "svc.fit(X_res,y_res)\n",
    "tr_pred= svc.predict(X_res)\n",
    "print(accuracy_score(tr_pred,y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1023    2]\n",
      " [   0  509]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(tr_pred,y_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its clear that model is performing excellent on training, still nevertheless performance on the test is slighly lower than the one on train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf= RandomForestClassifier()\n",
    "rf.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3= rf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models accuracy is 0.9341825902335457\n"
     ]
    }
   ],
   "source": [
    "print('The models accuracy is', accuracy_score(y_pred3,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[440  31]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.93      0.97       471\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.93       471\n",
      "   macro avg       0.50      0.47      0.48       471\n",
      "weighted avg       1.00      0.93      0.97       471\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred3,y_test))\n",
    "print(classification_report(y_pred3,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inherent problem of the dataset is that the class imbalance is unquestionably hig, hence as an inevitable outcome one particular class has the highest number of datapoints classified under it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a secondary objective of the problem statement dimensionality reduction must be performed and then assess if we get any further improvement in the models predictive accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca= PCA().fit(X_res)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('No.of Components')\n",
    "plt.ylabel('Cumulative Explained Variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=200, svd_solver='arpack')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can choose 'arpack' because we have verified the condition 0 < n_components < min(X.shape)\n",
    "# because we have 0 < 200 < 563\n",
    "pca=PCA(n_components=200,svd_solver='arpack')\n",
    "pca.fit(X_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1534, 200)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_dat= pca.transform(X_res)\n",
    "pca_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(pca_dat,y_res,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers=[\n",
    "    LogisticRegression(),\n",
    "    SVC(),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X_train,X_test,y_train,y_test):\n",
    "    for clf in Classifiers:\n",
    "        s=time.time()\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred= clf.predict(X_test)\n",
    "        acc=accuracy_score(y_pred,y_test)\n",
    "        e=time.time()\n",
    "        print(f\"Accuracy: {round(acc,3)} \\t Time(in secs): {round(e-s,3)} \\t Classifier: {clf.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.898 \t Time(in secs): 0.248 \t Classifier: LogisticRegression\n",
      "Accuracy: 0.98 \t Time(in secs): 0.3 \t Classifier: SVC\n",
      "Accuracy: 0.963 \t Time(in secs): 1.144 \t Classifier: RandomForestClassifier\n",
      "Accuracy: 0.989 \t Time(in secs): 5.247 \t Classifier: XGBClassifier\n"
     ]
    }
   ],
   "source": [
    "accuracy(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the models XG Boost classifier was the best performer in terms of accuracy score,but however it was inefficient with contrast to SVC as it consumed about more than 5 minutes. SVC performed optimally as  therefore we will choose this model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
